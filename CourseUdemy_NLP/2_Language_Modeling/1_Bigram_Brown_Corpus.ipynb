{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "- A model of probability of a sequence of words(sentence)\n",
    "    + Unigram\n",
    "    + Bigram\n",
    "    + Trigram\n",
    "    + Ngram\n",
    "\n",
    "## Bigram\n",
    "- 2 consecutive words in a sentence\n",
    "    - Eg.: \"The quick brown fox jumps over the lazy dog.\" -> Bigram:\n",
    "        + The quick\n",
    "        + Quick brown\n",
    "        + Brown fox\n",
    "        + Fox jumps\n",
    "        + ...\n",
    "- Bigram model: Probability of 2 word appear together\n",
    "$$p(w_t|w_{t-1})$$\n",
    "    - Eg:\n",
    "        + p(brown|quick) = 0.5\n",
    "        + p(the|the) = 0\n",
    "- How to build bigram model: p(brown|quick)\n",
    "    - Count how many times 'quick' -> 'brown' appear\n",
    "    - Count how many times 'quick' appear\n",
    "$$p(brown|quick) = \\frac{count(quick \\rightarrow brown)}{count(quick)}$$\n",
    "\n",
    "## Language model Bayes Rule\n",
    "$$p(ABC) = p(C|AB)p(AB) = p(C|AB)p(B|A)p(A)$$\n",
    "- Trigram\n",
    "$$p(C|AB) = \\frac{count(ABC)}{count(AB)}$$\n",
    "- Bigram\n",
    "$$p(B|A) = \\frac{count(AB)}{count(A)}$$\n",
    "- Unigram\n",
    "$$p(A) = \\frac{count(A)}{corpus\\ length}$$\n",
    "\n",
    "#### Longer sentence\n",
    "- Longer sentence\n",
    "$$p(ABCDE) = p(E|ABCD)p(D|ABC)p(C|AB)p(B|A)p(A)$$\n",
    "- Long sentence with only Bigram(Break down the sentence -> more phrase are capable)\n",
    "$$p(ABCDE) = p(E|D)p(D|C)p(C|B)p(B|A)p(A)$$\n",
    "\n",
    "#### Problems with Language model\n",
    "- p(dog|the quick brown fox jumps over the) > 0: because it appear in the document\n",
    "- p(turtle|the quick brown fox jumps over the) = 0: because it not appear but tutle is still valid\n",
    "- Solution: Smoothing\n",
    "$$p_{smooth}(B|A) = \\frac{count(AB) + 1}{count(A) + V}$$\n",
    "    + V: vocab size = number of distinct words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "KEEP_WORDS = set([\n",
    "    'king', 'man', 'queen', 'woman',\n",
    "    'italy', 'rome', 'france', 'paris',\n",
    "    'london', 'britain', 'england',\n",
    "])\n",
    "\n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
    "    # returns 57340 of the Brown corpus\n",
    "    # each sentence is represented as a list of individual string tokens\n",
    "    sentences = brown.sents()\n",
    "    indexed_sentences = []\n",
    "\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "\n",
    "    word_idx_count = {\n",
    "        0: float('inf'),\n",
    "        1: float('inf'),\n",
    "    }\n",
    "\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "\n",
    "            # keep track of counts for later sorting\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    # restrict vocab size\n",
    "\n",
    "    # set all the words I want to keep to infinity\n",
    "    # so that they are included when I pick the most\n",
    "    # common words\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "    sorted_word_idx_count = sorted(\n",
    "        word_idx_count.items(),\n",
    "        key=operator.itemgetter(1),\n",
    "        reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [\n",
    "                idx_new_idx_map[idx] if idx in idx_new_idx_map \n",
    "                else unknown \n",
    "                    for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 3111, 21, 13, 249, 26, 172, 893, 18, 6629, 27, 38, 315, 15]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 57013\n",
      "Vocab size: 10001\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(10000)\n",
    "\n",
    "# vocab size\n",
    "V = len(word2idx)\n",
    "\n",
    "display(sentences[10])\n",
    "print('Number of sentences:', len(sentences))\n",
    "print('Vocab size:', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse index\n",
    "idx2word = dict((v, k) for k, v in word2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_sentence(sentence):\n",
    "    return ' '.join(idx2word[i] for i in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the fulton county grand jury said friday an investigation of UNKNOWN recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rebuild sentences[0]\n",
    "rebuild_sentence(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the jury further said in UNKNOWN UNKNOWN that the city executive committee , which had over-all charge of the election , `` deserves the praise and thanks of the city of atlanta '' for the manner in which the election was conducted .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rebuild sentences[1]\n",
    "rebuild_sentence(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$p_{smooth}(B|A) = \\frac{count(AB) + smoothing}{count(A) + V}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=1):\n",
    "    '''\n",
    "    structure of bigram probability matrix will be:\n",
    "        (last word, current word) --> probability\n",
    "    we will use add-1 smoothing\n",
    "    note: we'll always ignore this from the END token\n",
    "    '''\n",
    "    bigram_probs = np.ones((V, V)) * smoothing\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                # beginning word\n",
    "                bigram_probs[start_idx, sentence[i]] += 1\n",
    "            else:\n",
    "                # middle word\n",
    "                bigram_probs[sentence[i-1], sentence[i]] += 1\n",
    "\n",
    "            # if we're at the final word\n",
    "            # we update the bigram for last -> current\n",
    "            # AND current -> END token\n",
    "            if i == len(sentence) - 1:\n",
    "                # final word\n",
    "                bigram_probs[sentence[i], end_idx] += 1\n",
    "\n",
    "    # normalize the counts along the rows to get probabilities\n",
    "    bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 10001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a matrix where:\n",
    "# row = last word\n",
    "# col = current word\n",
    "# value at [row, col] = p(current word | last word)\n",
    "bigram_probs = get_bigram_probs(\n",
    "    sentences, V, \n",
    "    start_idx=word2idx['START'],\n",
    "    end_idx=word2idx['END'],\n",
    "    smoothing=0.1)\n",
    "\n",
    "bigram_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probability score\n",
    "    + p in range [0,1]\n",
    "## $$p(w_1, w_2, ..., w_T) = p(w_1)\\prod_{t=2}^{T}p(w_t|w_{t-1})$$\n",
    "\n",
    "- Log Probability score\n",
    "    + alway negative (< 0)\n",
    "    + shorter sentences have high log prob score\n",
    "## $$log(p(w_1, w_2, ..., w_T)) = log(p(w_1)) + \\sum_{t=2}^{T}log(p(w_t|w_{t-1}))$$\n",
    "\n",
    "- Normalized Log Probability score\n",
    "    + alway negative (< 0)\n",
    "    + Long sentences ~ Short sentences score \n",
    "## $$\\frac{1}{T}log(p(w_1, w_2, ..., w_T)) = \\frac{1}{T}[log(p(w_1)) + \\sum_{t=2}^{T}log(p(w_t|w_{t-1}))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']\n",
    "\n",
    "## Normalized Log Probability\n",
    "def get_score(sentence):\n",
    "    '''\n",
    "    a function to calculate normalized log prob score for a sentence\n",
    "    '''\n",
    "    score = 0\n",
    "    for i in range(len(sentence)):\n",
    "        if i == 0:\n",
    "            # beginning word\n",
    "            score += np.log(bigram_probs[start_idx, sentence[i]])\n",
    "        else:\n",
    "            # middle word\n",
    "            score += np.log(bigram_probs[sentence[i-1], sentence[i]])\n",
    "    # final word\n",
    "    score += np.log(bigram_probs[sentence[-1], end_idx])\n",
    "\n",
    "    # normalize the score\n",
    "    return score / (len(sentence) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate real sentences (from Brown corpus dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you are still in france '' .\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-3.921470607899102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "real_idx = np.random.choice(len(sentences))\n",
    "real = sentences[real_idx]\n",
    "\n",
    "display(rebuild_sentence(real))\n",
    "display(get_score(real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in fact , the whole generation of the founding fathers of UNKNOWN -- UNKNOWN , monk , davis , UNKNOWN , and the rest -- are just now at a considerable discount .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-4.8694435452005305"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "real_idx = np.random.choice(len(sentences))\n",
    "real = sentences[real_idx]\n",
    "\n",
    "display(rebuild_sentence(real))\n",
    "display(get_score(real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the UNKNOWN screw machine , now known as the brown & sharpe hand screw machine , takes its ancestry directly from mr. brown's efforts to introduce equipment to simplify the manufacture of the sewing machine .\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-5.667113926377538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "real_idx = np.random.choice(len(sentences))\n",
    "real = sentences[real_idx]\n",
    "\n",
    "display(rebuild_sentence(real))\n",
    "display(get_score(real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate fake sentences (random words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probable dug roar lyrics coming'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-9.12586696592133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake = np.random.choice(V, size=5)\n",
    "\n",
    "display(rebuild_sentence(fake))\n",
    "display(get_score(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"coins grace 1954 leather simplify suffered victor plato's fibers authenticity\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-9.600882899836927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake = np.random.choice(V, size=10)\n",
    "\n",
    "display(rebuild_sentence(fake))\n",
    "display(get_score(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uncle vivid carla cups fortunately deeply resources amusing concede manufacture saved liquor oh book consonantal'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-9.248811743297562"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake = np.random.choice(V, size=15)\n",
    "\n",
    "display(rebuild_sentence(fake))\n",
    "display(get_score(fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate custom sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_custom_sentence(custom):\n",
    "    custom = custom.lower().split()\n",
    "\n",
    "    bad_sentence = False\n",
    "    for token in custom:\n",
    "        if token not in word2idx:\n",
    "            bad_sentence = True\n",
    "\n",
    "    if bad_sentence:\n",
    "        print(\"Sorry, you entered words that are not in the vocabulary\")\n",
    "    else:\n",
    "        # convert sentence into list of indexes\n",
    "        custom = [word2idx[token] for token in custom]\n",
    "        print(\"SCORE:\", get_score(custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: -7.512908229477057\n"
     ]
    }
   ],
   "source": [
    "my_sentence = 'my name is Peter'\n",
    "evaluate_custom_sentence(my_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: -7.130362607114305\n"
     ]
    }
   ],
   "source": [
    "my_sentence = 'Go to school'\n",
    "evaluate_custom_sentence(my_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: -5.73929340860894\n"
     ]
    }
   ],
   "source": [
    "my_sentence = 'I love you so much do you know'\n",
    "evaluate_custom_sentence(my_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
