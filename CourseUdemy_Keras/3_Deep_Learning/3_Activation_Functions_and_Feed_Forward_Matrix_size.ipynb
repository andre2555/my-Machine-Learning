{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- Non-linear functions\n",
    "- Applied to the outputs of previous layer -> next layer or final layer\n",
    "- Make the network non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "$$y = \\frac{1}{1 + e^{-x}}$$\n",
    "- Value return in range [0, 1]\n",
    "<img src=\"./Figs/10.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax - Multiclass Classification\n",
    "- Return a list of values(in range [0,1]): Sum up to 1\n",
    "\n",
    "$$\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K}e^{z_k}},\\ for\\ j=1,...,K$$\n",
    "\n",
    "- Softmax as activation function\n",
    "$$z_i = \\sum_jw_{i,j}x_j + b_i$$\n",
    "$$y = softmax(z)_i = \\frac{e^{z_i}}{\\sum_je^{z_j}}$$\n",
    "\n",
    "<img src=\"./Figs/17.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step\n",
    "- Value return = 0 or 1\n",
    "<img src=\"./Figs/11.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    return x > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh\n",
    "- Value return in range [-1, 1]\n",
    "<img src=\"./Figs/12.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu\n",
    "$$y = max(0,x)$$\n",
    "- Value return in range [0, +oo]\n",
    "- Emphasize the value\n",
    "- Increase trainning speed\n",
    "<img src=\"./Figs/13.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softplus\n",
    "$$y = log(1 + e^x)$$\n",
    "- Value return in range [0, +oo]\n",
    "- smoother near 0 than relu\n",
    "<img src=\"./Figs/14.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return log(1.0 + np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected - Feed forward - Matrix size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Layer 1\n",
    "<img src=\"./Figs/15.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "- Layer 2\n",
    "<img src=\"./Figs/16.jpg\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
