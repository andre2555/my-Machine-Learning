{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxfsNm0ODIPN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvQI74oEnifU"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OUuHjWVMXsQ"
   },
   "source": [
    "### Bag of Word\n",
    "\n",
    "- Text\n",
    "\n",
    "```\n",
    "  John likes ice cream\n",
    "  John hates chocolate.\n",
    "  ```\n",
    "  \n",
    "  - Create index table\n",
    "  {'John': 0, 'chocolate': 1, 'cream': 2, 'hates': 3, 'ice': 4, 'likes': 5}\n",
    "  \n",
    "  - Tokenization: Count number of words\n",
    "  \n",
    "  ```\n",
    "    ['John': 1, 'chocolate': 0, 'cream': 1, 'hates': 0, 'ice': 1, 'likes': 1]\n",
    "    ['John': 1, 'chocolate': 1, 'cream': 0, 'hates': 1, 'ice': 0, 'likes': 0]\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLnLR6zaGM4y"
   },
   "outputs": [],
   "source": [
    "sentences = ['John likes ice cream', 'John hates chocolate.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ToJ4_PKmMExq",
    "outputId": "080fa594-09a4-4885-9f9e-1cd879374d2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the sentences\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(sentences)\n",
    "\n",
    "# Vocabulary serves as an index of each word\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "d8DEe7QEMLP1",
    "outputId": "3c57c26d-496f-4a74-b223-b02887e67235"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization: Transform into number-of-word array\n",
    "vectorizer.transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6m0v6vth2gL"
   },
   "source": [
    "### One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "cmXJznMFdsg5",
    "outputId": "47350a2f-8cf4-470f-bfa1-fc1eaf59ec6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London', 'Berlin', 'Berlin', 'New York', 'London']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sklearn - One hot encoding\n",
    "cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\n",
    "display(cities)\n",
    "\n",
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "city_labels = label_encoder.fit_transform(cities)\n",
    "display(city_labels)\n",
    "\n",
    "# one hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "cities_onehot = onehot_encoder.fit_transform(\n",
    "    city_labels.reshape((len(city_labels), 1)))\n",
    "display(cities_onehot)\n",
    "\n",
    "# Invert\n",
    "from numpy import argmax\n",
    "\n",
    "inverted = np.array([argmax(city) for city in cities_onehot])\n",
    "display(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "JHvRGNKGoL7w",
    "outputId": "3440f483-ea4e-4261-b311-ae78be328922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London', 'Berlin', 'Berlin', 'New York', 'London']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras - One hot encoding\n",
    "cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\n",
    "display(cities)\n",
    "\n",
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "city_labels = label_encoder.fit_transform(cities)\n",
    "display(city_labels)\n",
    "\n",
    "# one hot Encoding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "cities_onehot = to_categorical(city_labels)\n",
    "display(cities_onehot)\n",
    "\n",
    "# Invert\n",
    "from numpy import argmax\n",
    "\n",
    "inverted = np.array([argmax(city) for city in cities_onehot])\n",
    "display(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tG5TeW2doTWW"
   },
   "source": [
    "### Word Embedding\n",
    "- Unknown words (words that are not in the vocabulary) are denoted in Keras with word_count + 1\n",
    "#### Word Embedding vs BoW\n",
    "- Bag of words: sequence of words = a single feature vector.\n",
    "- Word Embedding:\n",
    "    + Words represented by each word as a vector\n",
    "    + Characters represented by each character as a vector\n",
    "    + N-grams of words/characters represented as a vector (N-grams are overlapping groups of multiple succeeding words/characters in the text)\n",
    "\n",
    "#### Word Embedding vs Onehot\n",
    "- one-hot encoding = hardcoded\n",
    "- Word Embedding = softcoded\n",
    "    + collect more information into fewer dimensions\n",
    "    + map semantic meaning into a geometric space(**embedding space**)\n",
    "\n",
    "#### Train Word Embedding\n",
    "- Train during Neural Net\n",
    "- Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7tsDjlVt_TB"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'John likes ice cream',\n",
    "    'John hates chocolate',\n",
    "    'Kathy likes Google',\n",
    "    'Kathy hates Apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-WhoGVGv6s1"
   },
   "source": [
    "#### Tokenize\n",
    "- vectorize a text corpus into a list of integers\n",
    "- Each integer maps to a value in a dictionary that encodes the entire corpus\n",
    "- index 0 is reserved and is not assigned to any word\n",
    "- Indexing is ordered after the most common words in the text (1 = most common)\n",
    "\n",
    "num_words: responsible for setting the size of the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "AXls2Ze7oUBz",
    "outputId": "923053e5-262d-49a5-e0fd-ebc3af36bbc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'john': 1,\n",
       " 'likes': 2,\n",
       " 'hates': 3,\n",
       " 'kathy': 4,\n",
       " 'ice': 5,\n",
       " 'cream': 6,\n",
       " 'chocolate': 7,\n",
       " 'google': 8,\n",
       " 'apple': 9}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocab size: 10\n",
      "\n",
      "John likes ice cream\n",
      "[1, 2, 5, 6]\n",
      "\n",
      "John hates chocolate\n",
      "[1, 3, 7]\n",
      "\n",
      "Kathy likes Google\n",
      "[4, 2, 8]\n",
      "\n",
      "Kathy hates Apple\n",
      "[4, 3, 9]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Fit tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Word index\n",
    "display(tokenizer.word_index)\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "print('\\nVocab size: {}'.format(vocab_size))\n",
    "\n",
    "# Transform\n",
    "sentences_trained = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print('')\n",
    "    print(sentences[i])\n",
    "    print(sentences_trained[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62KdoFUMHxPw"
   },
   "source": [
    "#### pad_sequences\n",
    "text sequence has in most cases different length of words\n",
    "\n",
    "    -> pads the sequence of words with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "M4zMKwrN_wic",
    "outputId": "4040a053-4194-4706-930a-382618ba593a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 6]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 20\n",
    "\n",
    "X_train = pad_sequences(sentences_trained, padding='post', maxlen=maxlen)\n",
    "display(X_train.shape)\n",
    "display(sentences_trained[0])\n",
    "display(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P62NqCkLJVi9"
   },
   "source": [
    "#### Train during Neural Net\n",
    "weights of the embedding layer are initialized with random weights and are then adjusted through backpropagation during training\n",
    "\n",
    "- `input_dim`: the size of the vocabulary\n",
    "- `output_dim`: the size of the dense vector\n",
    "- `input_length`: the length of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "2zle_sylJYI9",
    "outputId": "9e6e5151-046e-44f2-cf38-51182c1ca236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            500       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 500\n",
      "Trainable params: 500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size, \n",
    "    output_dim=embedding_dim, \n",
    "    input_length=maxlen))\n",
    "\n",
    "# using a MaxPooling1D/AveragePooling1D \n",
    "#     or a GlobalMaxPooling1D/GlobalAveragePooling1D\n",
    "#     after the embedding layer\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlnYI8H_hnP3"
   },
   "source": [
    "#### Use Pretrained dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "e-O5LMUoMZ8-",
    "outputId": "41b0e53f-cfc3-4a0c-d314-9f52e986d4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-27 11:56:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2018-10-27 11:56:43--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.zip’\n",
      "\n",
      "glove.zip           100%[===================>] 822.24M  1.27MB/s    in 8m 34s  \n",
      "\n",
      "2018-10-27 12:05:18 (1.60 MB/s) - ‘glove.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.zip\n",
      "  inflating: ./glove/glove.6B.50d.txt  \n",
      "  inflating: ./glove/glove.6B.100d.txt  \n",
      "  inflating: ./glove/glove.6B.200d.txt  \n",
      "  inflating: ./glove/glove.6B.300d.txt  \n",
      "glove.6B.100d.txt  glove.6B.200d.txt  glove.6B.300d.txt  glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained Glove dataset\n",
    "! wget http://nlp.stanford.edu/data/glove.6B.zip -O glove.zip\n",
    "! unzip glove.zip -d ./glove\n",
    "! rm -rf glove.zip\n",
    "! ls glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NjXUYDqBonSS",
    "outputId": "f479fd55-eeb1-4db5-f8b4-19ea881a673a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.04445\r\n"
     ]
    }
   ],
   "source": [
    "# Exam pretrained dataset\n",
    "! head -n 1 glove/glove.6B.50d.txt | cut -c-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP9P1J2tpOcg"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "x1T70u1fpYhs",
    "outputId": "d75ffe21-80ec-4751-92b3-ed43894a8335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage words covered 90.0%\n"
     ]
    }
   ],
   "source": [
    "# Extract embedding matrix\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    'glove/glove.6B.50d.txt',\n",
    "    tokenizer.word_index, embedding_dim)\n",
    "\n",
    "display(embedding_matrix.shape)\n",
    "\n",
    "# Check non-zero words (already exist in the pretrained dataset)\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print('Percentage words covered {}%'.format(\n",
    "    float(nonzero_elements) / vocab_size * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "0y-4fbger4Jn",
    "outputId": "24325bf1-9585-4207-dfc3-810efeca09c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 50)            500       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 500\n",
      "Trainable params: 0\n",
      "Non-trainable params: 500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(\n",
    "    input_dim=vocab_size, \n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix], \n",
    "    input_length=maxlen,\n",
    "    trainable=False))\n",
    "        ### trainable\n",
    "        ###    False: Not allow continous trainning Embedding\n",
    "        ###    True: Allow continous trainning Embedding\n",
    "\n",
    "# using a MaxPooling1D/AveragePooling1D \n",
    "#     or a GlobalMaxPooling1D/GlobalAveragePooling1D\n",
    "#     after the embedding layer\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
